<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Ahmad Mohammad</title>
    <link>http://ahmedas91.github.io/post/</link>
    <description>Recent content in Posts on Ahmad Mohammad</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 Apr 2016 20:55:56 +0300</lastBuildDate>
    <atom:link href="http://ahmedas91.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Classifying Students&#39; Success Rate</title>
      <link>http://ahmedas91.github.io/blog/2016/04/09/classifying-students-success-rate/</link>
      <pubDate>Sat, 09 Apr 2016 20:55:56 +0300</pubDate>
      
      <guid>http://ahmedas91.github.io/blog/2016/04/09/classifying-students-success-rate/</guid>
      <description>

&lt;p&gt;In this post we will analyze the dataset on students&amp;rsquo; performance and develop classification models that will predict the likelihood that a given student will pass and choose the the most effective model that uses the least amount of computation costs. The three models that explored here are logistic regression, naive bayes and random forests. I will start first with brief technical summary of the models (some machine learning background is assumed).&lt;/p&gt;

&lt;h3 id=&#34;classification-models:71774469e06971d2e0883966e500d1d4&#34;&gt;Classification Models&lt;/h3&gt;

&lt;h2 id=&#34;logistic-regression:71774469e06971d2e0883966e500d1d4&#34;&gt;Logistic Regression&lt;/h2&gt;

&lt;p&gt;Since the target values can have only two classes, passed and failed, where pass is the positive class and failed is the negative one and the features include both binarized categorical values and numeric values, the logistic regression model would be a suitable one here.&lt;/p&gt;

&lt;p&gt;The logistic regression is parametric, discriminative classification model and one of the most widely applied machine learning due to its low computational complexity of fitting the model parameters and high interpretability of the model (1). Since it is a classification model, it is used widely in many fields including image recognition, social science and medicine.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say we have an observation with &lt;code&gt;$p$&lt;/code&gt; features each, &lt;code&gt;$X=(x_{1}, x_{2},...,x_{p})$&lt;/code&gt;, and the vector &lt;code&gt;$Y=(y_{1}, y_{2}, ...,y_{c})$&lt;/code&gt; which states the binary class membership of the observed vector of predictors (in our case we only have &lt;code&gt;$c=2$&lt;/code&gt; classes). The logistic regression model calculates the class membership probability  for one of the two categories in the data set by the sigmoid function:&lt;/p&gt;

&lt;div&gt;$$p(y=1| X,W)=\frac { 1 }{ 1+{ exp }^{ -\left( { { w }_{ 0 }+\sum _{ i=1 }^{ p }{ { w }_{ i }{ x }_{ i } }  } \right)  } } $$&lt;/div&gt;

&lt;div&gt;$$p(y=0|X,W)=\frac { { exp }^{ -\left( { { w }_{ 0 }+\sum _{ i=1 }^{ p }{ { w }_{ i }{ x }_{ i } }  } \right)  } }{ 1+{ exp }^{ -\left( { { w }_{ 0 }+\sum _{ i=1 }^{ p }{ { w }_{ i }{ x }_{ i } }  } \right)  } } $$&lt;/div&gt;

&lt;p&gt;where &lt;code&gt;$W=(x_{0}, x_{1},x_{2},...,x_{p})$&lt;/code&gt; are the regression coefficients. The coefficients are estimated (or trained) by either maximizing the maximum likelihood estimation or minimizing the error function. Assuming we are going with the second option, the error function is:&lt;/p&gt;

&lt;div&gt;$$
E(w)=\frac { 1 }{ n } \sum _{ i=1 }^{ n } \left[ -{ y }^{ (i) }log\left( \frac { 1 }{ 1+{ exp }^{ -\left( { { w }_{ 0 }+\sum _{ i=1 }^{ p }{ { w }_{ i }{ x }_{ i } }  } \right)  } }  \right) -(1-{ y }^{ (i) })log\left( \frac { { exp }^{ -\left( { { w }_{ 0 }+\sum _{ i=1 }^{ p }{ { w }_{ i }{ x }_{ i } }  } \right)  } }{ 1+{ exp }^{ -\left( { { w }_{ 0 }+\sum _{ i=1 }^{ p }{ { w }_{ i }{ x }_{ i } }  } \right)  } }  \right)  \right]$$&lt;/div&gt;

&lt;p&gt;where &lt;code&gt;$n$&lt;/code&gt; is the total number of observation. One of the disadvantages of logistic regression is that it&amp;rsquo;s prone to overfitting. To avoid this, we can add either L1 or L2 regularization to the error function. The time complexity for this optimization problem with the number of observations, &lt;code&gt;$O(n)$&lt;/code&gt; and the space complexity &lt;code&gt;$O(nm)$&lt;/code&gt; where m is the number of features.&lt;/p&gt;

&lt;h2 id=&#34;gaussian-naive-bayes:71774469e06971d2e0883966e500d1d4&#34;&gt;Gaussian Naive Bayes&lt;/h2&gt;

&lt;p&gt;Because we are also looking for a model that uses the least amount of computation costs, the next model we will explore is the Guassian  Niave Bayes. It is one of the most computationally efficient and effective learning algirthims (3). Moreover, since administrators might want to predict the graduation rate of small groups of students, the model is also effective when the training size is small.&lt;/p&gt;

&lt;p&gt;Here, unlike logistic regression where the model tries to learn &lt;code&gt;$p(y|x)$&lt;/code&gt; directly, naive bayes model try to model &lt;code&gt;$p(x|y)$&lt;/code&gt; and &lt;code&gt;$p(y)$&lt;/code&gt;based on the assumption that the features are conditionally independent of each other. It is mostly used in text classification, medical diagnosis and spam filtering.&lt;/p&gt;

&lt;p&gt;Using the same notation as above where the feature vector &lt;code&gt;$X=(x_{1}, x_{2},...,x_{n})$&lt;/code&gt; and the class vector &lt;code&gt;$Y=(y_{1}, y_{2}, ...,y_{c})$&lt;/code&gt;, according to bayes rule the probability of getting the class &lt;code&gt;$y_{k}$&lt;/code&gt; given the feature vector &lt;code&gt;$X$&lt;/code&gt; is:&lt;/p&gt;

&lt;div&gt;$$p(y_{ k }|X)=\frac { p(X|{ y }_{ k })p({ y }_{ k }) }{ p(X) }$$&lt;/div&gt;

&lt;p&gt;Since the naive bayes assumes that the features are indepedent:&lt;/p&gt;

&lt;div&gt;$$p(X|y)=\prod _{ i=1 }^{ n }{ p({ x }_{ i }|{ y }_{ k }) }$$&lt;/div&gt;

&lt;p&gt;There fore the equation simplifies to:&lt;/p&gt;

&lt;div&gt;$$p({ y }_{ k }|X)=\frac { p({ y }_{ k })\prod _{ i=1 }^{ n }{ p({ x }_{ i }|{ y }_{ k }) }  }{ \sum _{ j=1 }^{ c }{ p({ y }_{ j })\prod _{ i=1 }^{ n }{ p({ x }_{ i }|{ y }_{ j }) }  }  } $$&lt;/div&gt;

&lt;p&gt;And then we calculate the probabilty of all the classis and the one with the highest probability is the predicted class given a set of features. This is known as the maximum a posteriori or MAP decision rule The simplified rule as:&lt;/p&gt;

&lt;div&gt;$$y={ \underset { { y }_{ k } }{ argmax }  }\ p({ y }_{ k })\prod _{ i=1 }^{ n }{ p({ x }_{ i }|{ y }_{ k }) } $$&lt;/div&gt;

&lt;p&gt;In the case of Gaussain Niave bayes, The likelihood of the features is assumed to be Gaussian:&lt;/p&gt;

&lt;div&gt;$$p({ x }_{ i }|y)=\frac { 1 }{ \sqrt { 2\pi { \sigma  }_{ { x }_{ i },y }^{ 2 } }  } { exp }{ \left( -\frac { { ({ x }_{ i }-{ \mu  }_{{ x }_{ i }, y }) }^{ 2 } }{ 2{ \sigma  }_{ { x }_{ i },y }^{ 2 } }  \right)  }$$&lt;/div&gt;

&lt;p&gt;for a more comrehensive explaiantion of the model, check out &lt;a href=&#34;https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf&#34;&gt;this chapter&lt;/a&gt; from Tom Micheal&amp;rsquo;s Machine learning book.&lt;/p&gt;

&lt;p&gt;One of the advantages of naive bayes is that, as mentioned above, it only requires a small amount of training data to estimate the parameters necessary for classification. It also fast to train and to classify with a time complexity of &lt;code&gt;$O(np)$&lt;/code&gt; where &lt;code&gt;$n$&lt;/code&gt; is the number of training examples and p is the number of features and space complexity of &lt;code&gt;$O(pq)$&lt;/code&gt; where q is values for each feature. Moreover, it can handle missing data pretty well. However, one main disadvantage of naive bayes is that it has strong feature independence assumptions and therefore can be fooled by correlated features. It also does not perform well on large data.&lt;/p&gt;

&lt;h2 id=&#34;random-forests:71774469e06971d2e0883966e500d1d4&#34;&gt;Random Forests&lt;/h2&gt;

&lt;p&gt;The current data only has the data of 395 students without any missing values. In the real world however, the data will be in hundred of thousands or even millions with many missing values. Therefore, we&amp;rsquo;re going to choose a model that can handle large datasets as well as missing values. This model is random forests.&lt;/p&gt;

&lt;p&gt;Random Forests model can be used for both regression and classification problems in wide range of applications ranging from biology(recognizing the important elements in protein sequences) to image recognition. One interesting application is predicting body movement in Microsoft&amp;rsquo;s Kinect.&lt;/p&gt;

&lt;p&gt;The model is an  aggregation (ensemble) of another weaker machine learning model, decision trees. One of the main issues with decision trees is that it is can easilty overfit the data. Random forests overcome this problem by using multiple decision trees and choosing the classification having the most votes from those trees. The procedure is as follows:
1. A bootstrapped sample is taken from the training set.
2. A random number of features are chosen to form a decision tree.
3. Each tree is trained and grown to the fullest extent possible without pruning.&lt;/p&gt;

&lt;p&gt;The steps above are repeated multiple times to random decision trees. Each tree gives a classification and the classification that has the most votes is chosen. The diagram below illustrate the process where we have n decision trees.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://ahmedas91.github.io/images/forests.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;Random forests is one most popular and best performing machine learning algorithms (2). The reason for their popularity is its excellent prediction performance, as well as its invariance to its feature units. Moreover, unlike logistic regression and naive bias, it can handle categorial data very well. However, one disadvantage is the it uses large memory when training. Its space complexity increases exponentially with a base of 2. It also take longer to train than the previous models where the time complexity is &lt;code&gt;$O(n(dm\log m))$&lt;/code&gt;. &lt;code&gt;$n$&lt;/code&gt;,&lt;code&gt;$d$&lt;/code&gt; and &lt;code&gt;$m$&lt;/code&gt; are the number of trees, the number of features and the number of instances, respectively. Moreover, random forests are difficult to interpret.&lt;/p&gt;

&lt;h2 id=&#34;exploring-the-data:71774469e06971d2e0883966e500d1d4&#34;&gt;Exploring the Data&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline  
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Read student data
student_data = pd.read_csv(&amp;quot;student-data.csv&amp;quot;)
# Note: The last column &#39;passed&#39; is the target/label, all other are feature columns
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute desired values - replace each &#39;?&#39; with an appropriate expression/function call
n_students = len(student_data)
n_features = student_data.shape[1] -1
n_passed = sum(student_data[&#39;passed&#39;]==&#39;yes&#39;)
n_failed = sum(student_data[&#39;passed&#39;]==&#39;no&#39;)
grad_rate = (n_passed/float(n_students)) *100
print &amp;quot;Total number of students: {}&amp;quot;.format(n_students)
print &amp;quot;Number of students who passed: {}&amp;quot;.format(n_passed)
print &amp;quot;Number of students who failed: {}&amp;quot;.format(n_failed)
print &amp;quot;Number of features: {}&amp;quot;.format(n_features)
print &amp;quot;Graduation rate of the class: {:.2f}%&amp;quot;.format(grad_rate)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prints:
Total number of students: 395
Number of students who passed: 265
Number of students who failed: 130
Number of features: 30
Graduation rate of the class: 67.09%&lt;/p&gt;

&lt;h2 id=&#34;preparing-the-data:71774469e06971d2e0883966e500d1d4&#34;&gt;Preparing the Data&lt;/h2&gt;

&lt;p&gt;In this section, we will prepare the data for modeling, training and testing.&lt;/p&gt;

&lt;h3 id=&#34;identify-feature-and-target-columns:71774469e06971d2e0883966e500d1d4&#34;&gt;Identify feature and target columns&lt;/h3&gt;

&lt;p&gt;It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s first separate our data into feature and target columns, and see if any features are non-numeric.&lt;br/&gt;
&lt;strong&gt;Note&lt;/strong&gt;: For this dataset, the last column (&lt;code&gt;&#39;passed&#39;&lt;/code&gt;) is the target or label we are trying to predict.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Extract feature (X) and target (y) columns
feature_cols = list(student_data.columns[:-1])  # all columns but last are features
target_col = student_data.columns[-1]  # last column is the target/label
print &amp;quot;Feature column(s):-\n{}&amp;quot;.format(feature_cols)
print &amp;quot;Target column: {}&amp;quot;.format(target_col)

X_all = student_data[feature_cols]  # feature values for all students
y_all = student_data[target_col]  # corresponding targets/labels
print &amp;quot;\nFeature values:-&amp;quot;
print X_all.head()  # print the first 5 rows
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;preprocess-feature-columns:71774469e06971d2e0883966e500d1d4&#34;&gt;Preprocess feature columns&lt;/h3&gt;

&lt;p&gt;The last code above shows that there are several non-numeric columns that need to be converted! Many of them are simply &lt;code&gt;yes&lt;/code&gt;/&lt;code&gt;no&lt;/code&gt;, e.g. &lt;code&gt;internet&lt;/code&gt;. These can be reasonably converted into &lt;code&gt;1&lt;/code&gt;/&lt;code&gt;0&lt;/code&gt; (binary) values.&lt;/p&gt;

&lt;p&gt;Other columns, like &lt;code&gt;Mjob&lt;/code&gt; and &lt;code&gt;Fjob&lt;/code&gt;, have more than two values, and are known as &lt;em&gt;categorical variables&lt;/em&gt;. The recommended way to handle such a column is to create as many columns as possible values (e.g. &lt;code&gt;Fjob_teacher&lt;/code&gt;, &lt;code&gt;Fjob_other&lt;/code&gt;, &lt;code&gt;Fjob_services&lt;/code&gt;, etc.), and assign a &lt;code&gt;1&lt;/code&gt; to one of them and &lt;code&gt;0&lt;/code&gt; to all others.&lt;/p&gt;

&lt;p&gt;These generated columns are sometimes called &lt;em&gt;dummy variables&lt;/em&gt;, and we will use the &lt;a href=&#34;http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies&#34;&gt;&lt;code&gt;pandas.get_dummies()&lt;/code&gt;&lt;/a&gt; function to perform this transformation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Preprocess feature columns
def preprocess_features(X):
    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty

    # Check each column
    for col, col_data in X.iteritems():
        # If data type is non-numeric, try to replace all yes/no values with 1/0
        if col_data.dtype == object:
            col_data = col_data.replace([&#39;yes&#39;, &#39;no&#39;], [1, 0])
        # Note: This should change the data type for yes/no columns to int

        # If still non-numeric, convert to one or more dummy variables
        if col_data.dtype == object:
            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. &#39;school&#39; =&amp;gt; &#39;school_GP&#39;, &#39;school_MS&#39;

        outX = outX.join(col_data)  # collect column(s) in output dataframe

    return outX

X_all = preprocess_features(X_all)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;split-data-into-training-and-test-sets:71774469e06971d2e0883966e500d1d4&#34;&gt;Split data into training and test sets&lt;/h3&gt;

&lt;p&gt;So far, we have converted all &lt;em&gt;categorical&lt;/em&gt; features into numeric values. In this next step, we split the data (both features and corresponding labels) into training and test sets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;num_all = student_data.shape[0]  # same as len(student_data)
num_train = 300  # about 75% of the data
num_test = num_all - num_train

# select features (X) and corresponding labels (y) for the training and test sets

from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=num_test, random_state=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;training-and-evaluating-models:71774469e06971d2e0883966e500d1d4&#34;&gt;Training and Evaluating Models&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Train a model
import time
from sklearn.metrics import f1_score


def train_classifier(clf, X_train, y_train):
    print &amp;quot;Training {}...&amp;quot;.format(clf.__class__.__name__)
    start = time.time()
    clf.fit(X_train, y_train)
    end = time.time()
    training_time = end-start
    return training_time

def predict_labels(clf, features, target):
    print &amp;quot;Predicting labels using {}...&amp;quot;.format(clf.__class__.__name__)
    start = time.time()
    y_pred = clf.predict(features)
    end = time.time()
    predicting_time = end-start
    f1 = f1_score(target.values, y_pred, pos_label=&#39;yes&#39;)
    return f1 , predicting_time

def f1_scores_times(clf):
    scores_training = []
    scores_testing = []
    training_times = []
    predicting_times = []
    for i in [100,200,300]:
        training_times.append(train_classifier(clf, X_train[:i], y_train[:i]))
        predicting_times.append(predict_labels(clf, X_train[:i], y_train[:i])[1])
        scores_training.append(predict_labels(clf, X_train[:i], y_train[:i])[0])
        scores_testing.append(predict_labels(clf, X_test, y_test)[0])
        
    return scores_training, scores_testing, training_times, predicting_times

def table(clf, name):
    raw_data = {
        &#39;F1 score for train set&#39;: f1_scores_times(clf)[0],
        &#39;F1 score for test set&#39;:  f1_scores_times(clf)[1],
        &#39;Training Time&#39;: f1_scores_times(clf)[2],
        &#39;Predicting Time&#39;:  f1_scores_times(clf)[3]}
    table = pd.DataFrame(raw_data,index=[100, 200, 300],
                          columns = [&#39;F1 score for train set&#39;, 
                                     &#39;F1 score for test set&#39;,
                                     &#39;Training Time&#39;, 
                                     &#39;Predicting Time&#39;])
    table.index.name = &#39;Training size&#39;
    table.columns.name = name
    #table = table.style.set_caption(name)
    
    return table
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ran_state = 0
    
from sklearn.linear_model import LogisticRegression
clf_logistic = LogisticRegression(penalty=&#39;l2&#39;, C=.5, solver= &#39;liblinear&#39;, random_state= ran_state)
from sklearn.naive_bayes import GaussianNB
clf_NB = GaussianNB()
from sklearn.ensemble import RandomForestClassifier
clf_RF = RandomForestClassifier(n_estimators=25)


table_logistic = table(clf_logistic, &#39;Logistic Regression&#39;)
table_NB = table(clf_NB, &#39;Gaussian Niave Bayes&#39;)
table_RF = table(clf_RF, &#39;Random Forests&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot():
    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,13))    #plt.figure(figsize=(15, 7))
    raw_data1 = {
            &#39;Logistic Regresstion&#39;: table_logistic[&#39;F1 score for train set&#39;],
            &#39;Gaussian Niave Bayes&#39;: table_NB[&#39;F1 score for train set&#39;],
            &#39;Random Forests&#39;: table_RF[&#39;F1 score for train set&#39;]}
    table1 = pd.DataFrame(raw_data1,index=[100, 200, 300],
                              columns = [&#39;Logistic Regresstion&#39;, &#39;Gaussian Niave Bayes&#39;,
                                        &#39;Random Forests&#39;])
    table1.index.name = &#39;Training size&#39;
    raw_data2 = {
            &#39;Logistic Regresstion&#39;: table_logistic[&#39;F1 score for test set&#39;],
            &#39;Gaussian Niave Bayes&#39;: table_NB[&#39;F1 score for test set&#39;],
            &#39;Random Forests&#39;: table_RF[&#39;F1 score for test set&#39;]}
    table2 = pd.DataFrame(raw_data2,index=[100, 200, 300],
                              columns = [&#39;Logistic Regresstion&#39;, &#39;Gaussian Niave Bayes&#39;,
                                        &#39;Random Forests&#39;])
    table2.index.name = &#39;Training size&#39;
    raw_data3 = {
            &#39;Logistic Regresstion&#39;: table_logistic[&#39;Training Time&#39;],
            &#39;Gaussian Niave Bayes&#39;: table_NB[&#39;Training Time&#39;],
            &#39;Random Forests&#39;: table_RF[&#39;Training Time&#39;]}
    table3 = pd.DataFrame(raw_data3,index=[100, 200, 300],
                              columns = [&#39;Logistic Regresstion&#39;, &#39;Gaussian Niave Bayes&#39;,
                                        &#39;Random Forests&#39;])
    table3.index.name = &#39;Training size&#39;
    raw_data4 = {
            &#39;Logistic Regresstion&#39;: table_logistic[&#39;Predicting Time&#39;],
            &#39;Gaussian Niave Bayes&#39;: table_NB[&#39;Predicting Time&#39;],
            &#39;Random Forests&#39;: table_RF[&#39;Predicting Time&#39;]}
    table4 = pd.DataFrame(raw_data4,index=[100, 200, 300],
                              columns = [&#39;Logistic Regresstion&#39;, &#39;Gaussian Niave Bayes&#39;,
                                        &#39;Random Forests&#39;])
    table4.index.name = &#39;Training size&#39;
    
    table1.plot(kind=&#39;bar&#39;,ax=axes[0,0],yticks=np.arange(0, 1.05, 0.05)).legend(bbox_to_anchor=(1, .21))
    axes[0,0].set_title(&#39;F1 Score For Training Set&#39;); axes[0,0].set_ylabel(&#39;F1 Score&#39;)
    table2.plot(kind=&#39;bar&#39;,ax=axes[0,1],yticks=np.arange(0, 1.05, 0.05)).legend(bbox_to_anchor=(1, .21))
    axes[0,1].set_title(&#39;F1 Score For Test Set&#39;); axes[0,1].set_ylabel(&#39;F1 Score&#39;)
    table3.plot(kind=&#39;line&#39;,ax=axes[1,0]).legend(bbox_to_anchor=(1, .5))
    axes[1,0].set_title(&#39;Training Time&#39;); axes[1,0].set_ylabel(&#39;Time(sec)&#39;)
    table4.plot(kind=&#39;line&#39;,ax=axes[1,1]).legend(bbox_to_anchor=(1, .5))
    axes[1,1].set_title(&#39;Predicting Time&#39;); axes[1,1].set_ylabel(&#39;Time(sec)&#39;)

plot()                         
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://ahmedas91.github.io/images/student_graph.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;h2 id=&#34;choosing-the-best-model:71774469e06971d2e0883966e500d1d4&#34;&gt;Choosing the Best Model&lt;/h2&gt;

&lt;p&gt;From the statistics above, logistic regression has the highest predicting performance on the test set for all training sizes. It is also the most efficient when it comes to predicting time. However, naive bayes is the most efficient when it comes to training time but its prediction accuracy is the worst on the training set. Random forests performs pretty well in on the test set and its prediction accuracy is actually not far from logistic regression. However, it&amp;rsquo;s the most computationally expensive in both training and predicting. Therefore, I would choose the logistic regression because of its excellent performance and time efficiency.&lt;/p&gt;

&lt;p&gt;In laymein terms, logistic regression makes predictions using a mathematical model that calculates the probability that a student passes or fails, where any probability below 0.5 means that the student failed and any probability above 0.5 means the student passed. This mathematical model is derived using the training data where the model is trained (or optimized) so that it gives the correct probabilities from the students&amp;rsquo;s features. To make a prediction, the trained model takes a student features and calculates the probability that he/she passes.&lt;/p&gt;

&lt;p&gt;The model can be improved by fine-tuning and optimising its paramters settings. The paramters here are the regulization methods, l1 or l2, and the stregth of the regulaization, a range of values from 0.1 to 1. This process requires doing a &lt;code&gt;GridSearch&lt;/code&gt; over the parameters and then get the ones that gives the highest f1 score.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import grid_search
from sklearn.metrics import make_scorer
from sklearn.cross_validation import StratifiedShuffleSplit

classifier = LogisticRegression()
c = np.arange(0.01,1.01,0.01)
parameters = {&#39;C&#39;: c, &#39;penalty&#39;:[&#39;l1&#39;,&#39;l2&#39;]}
f1_scorer = make_scorer(f1_score, pos_label=&amp;quot;yes&amp;quot;)
sss = StratifiedShuffleSplit(y_train, n_iter=10, test_size=0.24, random_state=ran_state)

reg = grid_search.GridSearchCV(classifier, param_grid=parameters, scoring=f1_scorer, cv=sss)
reg.fit(X_train, y_train)
clf = reg.best_estimator_
clf.get_params()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;f1_score(y_test,reg.predict(X_test), pos_label=&#39;yes&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prints:
0.79166666666666663&lt;/p&gt;

&lt;h3 id=&#34;references:71774469e06971d2e0883966e500d1d4&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;(1) E. Makalic and D. F. Schmidt. Review of Modern Logistic Regression Methods with Application to Small and Medium Sample Size Problems. Lecture Notes in Computer Science, &lt;sup&gt;6464&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2011&lt;/sub&gt;:2013-222,2011.&lt;/p&gt;

&lt;p&gt;(2) L. Breiman, “Random forests,” Machine Learning, vol. 4, no. 1, pp. 5–32, October 2001.&lt;/p&gt;

&lt;p&gt;(3) Zhang, Harry, and Jiang Su. &amp;ldquo;Naive Bayesian Classifiers for Ranking.&amp;rdquo; Machine Learning: ECML 2004 Lecture Notes in Computer Science (2004): 501-12.&lt;/p&gt;

&lt;p&gt;(4) Andrew Ng&amp;rsquo;s CS 229 &lt;a href=&#34;http://cs229.stanford.edu/notes/cs229-notes1.pdf&#34;&gt;course notes&lt;/a&gt; on supervised learning.&lt;/p&gt;

&lt;p&gt;(5) Makalic, Enes, and Daniel Francis Schmidt. &amp;ldquo;Review of Modern Logistic Regression Methods with Application to Small and Medium Sample Size Problems.&amp;rdquo; AI 2010: Advances in Artificial Intelligence Lecture Notes in Computer Science (2010): 213-22.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Boston Housing Prices</title>
      <link>http://ahmedas91.github.io/blog/2016/04/05/boston-housing-prices/</link>
      <pubDate>Tue, 05 Apr 2016 20:55:56 +0300</pubDate>
      
      <guid>http://ahmedas91.github.io/blog/2016/04/05/boston-housing-prices/</guid>
      <description>&lt;iframe src=&#34;https://nbviewer.jupyter.org/gist/ahmedas91/f93dc91562c06549f5890924d50d7c61&#34; width=&#34;1000&#34; height=&#34;6800&#34;&gt;&lt;/iframe&gt; 
</description>
    </item>
    
    <item>
      <title>Hunting Down Growth Stocks</title>
      <link>http://ahmedas91.github.io/blog/2016/04/05/hunting-down-growth-stocks/</link>
      <pubDate>Tue, 05 Apr 2016 20:55:56 +0300</pubDate>
      
      <guid>http://ahmedas91.github.io/blog/2016/04/05/hunting-down-growth-stocks/</guid>
      <description>

&lt;p&gt;Growth companies are those with huge potential returns and are often found in the technology sector. Here is the official definition from &lt;a href=&#34;http://www.investopedia.com/terms/g/growthcompany.asp&#34;&gt;investing.com&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A growth company is any firm whose business generates significant positive cash flows or earnings, which increase at significantly faster rates than the overall economy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So how can we spot those kind of companies? We can screan stocks based on annual earning growth, revenues growth, return on equity&amp;hellip;etc. We can also look for companies developing disruptive technologies. But could we just let the experts do it for us? for free? Well this is what I am exploring in this post.&lt;/p&gt;

&lt;p&gt;Each year, a bunch of news sites and organizations publish lists of the most innovative companies. In this post, I aggregated those lists from the sites and compared the holding period return for the listed companies from 2012 to 2015. Not all companies in the lists were included. Only those with that are listed on either NASDAQ, NYSE and AMEX, and Trading in the stock market during the whole holding period. You can download the aggregated list from the repo on github. The data are collected from the below soursec. Note that for Forves list, I could not find the whole list for 2012, only the top ten.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.forbes.com/sites/samanthasharf/2012/09/05/the-ten-most-innovative-companies-in-america/#3f28c5aa23d3&#34;&gt;Forbes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.businessinsider.com/presenting-the-39-companies-that-will-win-through-innovation-2012-4?op=1&#34;&gt;Barclays&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://top100innovators.stateofinnovation.thomsonreuters.com/&#34;&gt;Thomson Reuters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.strategyand.pwc.com/global/home/what-we-think/innovation1000/top-innovators-spenders#/tab-2012&#34;&gt;PWC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www2.technologyreview.com/tr50/2012/?_ga=1.224498527.453581319.1458158445&#34;&gt;MIT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.bcgperspectives.com/content/interactive/innovation_growth_most_innovative_companies_interactive_guide/&#34;&gt;BCG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.fastcompany.com/section/most-innovative-companies-2012&#34;&gt;Fast Company&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;loading-the-data:41fdf1a60fa155412b6923f1b832c365&#34;&gt;loading the data&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# first we import the required libraries
import pandas as pd
import pandas_datareader.data as web
import datetime
import matplotlib.pyplot as plt
%matplotlib inline  
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the aggregated list of companies
data = pd.read_csv(&amp;quot;TOP_COMP_2012.csv&amp;quot;)
start = datetime.datetime(2013, 1, 1)
end = datetime.datetime(2015, 12, 31)
tickers = list(data[&#39;ticker&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# importing the stock prices from yahoo finance
ls_key = &#39;Adj Close&#39;
f = web.DataReader(tickers, &#39;yahoo&#39;,start,end)
cleanData = f.ix[ls_key]
stock_data = pd.DataFrame(cleanData)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;results:41fdf1a60fa155412b6923f1b832c365&#34;&gt;Results&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Calulating the holding period returns 
returns = (stock_data.iloc[-1]/stock_data.iloc[0] - 1)
returns = pd.DataFrame(returns)
returns = returns.sort_values(by = [0], ascending=False)
returns[&#39;ticker&#39;] = list(returns.index)
returns.columns = [&#39;HPR&#39;,&#39;ticker&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Average return
avg_return = returns[&#39;HPR&#39;].mean(axis=0)*100
# percentage of positive returns
percentage_positive = len(returns[returns[&#39;HPR&#39;]&amp;gt;0])/float(len(returns))*100 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, iplot
init_notebook_mode()
iplot({
&amp;quot;data&amp;quot;: [
    go.Bar(
        x=returns[&#39;ticker&#39;],
        y=returns[&#39;HPR&#39;]
    )
        ],
&#39;layout&#39;: go.Layout(yaxis=go.YAxis(title=&#39;Holding Period Return&#39;, tickformat=&#39;%&#39;), 
                    autosize=False,
                    width=850,
                    height=600)
    })
&lt;/code&gt;&lt;/pre&gt;

&lt;iframe width=&#34;900&#34; height=&#34;800&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;https://plot.ly/~ahmedas91/0.embed&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;The above chart shows an astonishing results with an average return of 66%. Around 87% of the companies showed a positive holding period return. So can we conclude that we can just rely on the experts for hunting big growth companies? Let&amp;rsquo;s not get our hopes up yet. Let&amp;rsquo;s first check if we just invested in the S&amp;amp;P 500 and compare its cumulative returns with a portfolio of equal weights of the stocks above.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-pyhon&#34;&gt;# Importing the s&amp;amp;p price index
ff = web.DataReader(&amp;quot;^GSPC&amp;quot;, &#39;yahoo&#39;,start,end)
s_p = pd.DataFrame(ff[&#39;Adj Close&#39;])

# calculating the daily cumulative returns during the period
sp = pd.DataFrame([0])
portfolio = pd.DataFrame([0])
for i in range(1,len(stock_data)):
    sp_returns = (s_p.iloc[i]/s_p.iloc[0] - 1)[0]
    portfolio_returns = (stock_data.iloc[i]/stock_data.iloc[0] - 1).mean(axis=0)
    sp = sp.append([sp_returns])
    portfolio = portfolio.append([portfolio_returns])    
cum_returns = pd.concat([portfolio, sp], axis=1)
cum_returns.columns = [&#39;Portfolio&#39;,&#39;S&amp;amp;P&#39;]
cum_returns = cum_returns.set_index(stock_data.index)

iplot({
&#39;data&#39;:[
    go.Scatter(
            x = cum_returns.index,
            y = cum_returns[col],
            name = col) for col in cum_returns.columns],
&#39;layout&#39;: go.Layout(yaxis=go.YAxis(title=&#39;Holding Period Return&#39;, tickformat=&#39;%&#39;), 
            autosize=False,
            width=950,
            height=600)
})
&lt;/code&gt;&lt;/pre&gt;

&lt;iframe width=&#34;900&#34; height=&#34;800&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;https://plot.ly/~ahmedas91/4.embed&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;Well, we&amp;rsquo;re still beating the market by about 30%. So can we really just let the experts do it for us? Well maybe use their lists as preliminary screener only.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kaggle Digits Recognition</title>
      <link>http://ahmedas91.github.io/blog/2016/04/05/kaggle-digits-recognition/</link>
      <pubDate>Tue, 05 Apr 2016 20:55:56 +0300</pubDate>
      
      <guid>http://ahmedas91.github.io/blog/2016/04/05/kaggle-digits-recognition/</guid>
      <description>

&lt;p&gt;On my test post, we&amp;rsquo;ll solve  Kaggle&amp;rsquo;s &lt;a href=&#34;https://www.kaggle.com/c/digit-recognizer&#34;&gt;Digit Recognizer&lt;/a&gt; competition using python&amp;rsquo;s machine learning library &lt;code&gt;sklearn&lt;/code&gt;. It a really simple problem and used as a starting point (along with the &lt;a href=&#34;https://www.kaggle.com/c/titanic&#34;&gt;Titanic&lt;/a&gt; one) Kaggle competitions. If you want to check out an implementation from scratch, I have uploaded one on this repo on github.&lt;/p&gt;

&lt;h2 id=&#34;exploring-the-data:d0e6025616b3fd7d9f31e36bec92e790&#34;&gt;Exploring the data&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

X =  np.genfromtxt(&#39;train.csv&#39;,dtype=&#39;int_&#39;, 
                   delimiter=&#39;,&#39;, skip_header=1)
x_test =  np.genfromtxt(&#39;test.csv&#39;,dtype=&#39;int_&#39;, 
                        delimiter=&#39;,&#39;, skip_header=1)
x_train = X[:,1:]
y_train = X[:,0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each row in the &lt;code&gt;x_train&lt;/code&gt; and &lt;code&gt;x_test&lt;/code&gt; data is a 28x28 pixels image with a total of 784 pixels. Therefore, we will write a simple function takes randomly selected rows, reshapes them into 28x28 matrices and display them using &lt;code&gt;matplotlib.image.mpimg&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
%matplotlib inline

def display(n):    
    for i in range(1,(n**2)+1):
        plt.subplot(n,n,i)
        plt.axis(&#39;off&#39;)
        pic = np.reshape(x_train[np.random.randint(1,42000)],(28,28))
        imgplot = plt.imshow(pic, cmap=&#39;Greys&#39;)
display(5)
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://ahmedas91.github.io/images/digits.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;h2 id=&#34;training-the-data:d0e6025616b3fd7d9f31e36bec92e790&#34;&gt;Training the data&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;re going to use a cross validated logistic linear regression function with an l2 regularization using sklearn&amp;rsquo;s &lt;code&gt;linear_model.LogisticRegressionCV&lt;/code&gt;. Since we have 10 classes 0 to 9, we&amp;rsquo;ll also need the &lt;code&gt;multiclass.OneVsRestClassifier&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegressionCV

classifier = OneVsRestClassifier(LogisticRegressionCV(penalty=&#39;l2&#39;, n_jobs = -1)) 
classifier.fit(x_train, y_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s check the accuracy of the training data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# predict y using the train set
y_predicted = classifier.predict(x_train)             
accuracy = np.mean((y_predicted == y_train) * 100)
print &amp;quot;Training set accuracy: {0:.4f}%&amp;quot;.format(accuracy)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;submitting-the-data:d0e6025616b3fd7d9f31e36bec92e790&#34;&gt;Submitting the data&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;re gonna do the same thing with but with the x_test data and do some data cleaning for submission.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# predict y using the test set
y_test = classifier.predict(x_test) 
# kaggles requires the submission to include an index column         
index = np.arange(1,len(x_test)+1, dtype=int)  
# merging the y_test and index columns        
y_test = np.column_stack((index,y_test))
# convert to pandas dataframe
y_test = pd.DataFrame(y_test)
# headers required for the submission                          
y_test.columns = [&#39;ImageId&#39;,&#39;Label&#39;]    
# write the data to csv file in the directory               
y_test.to_csv(&#39;y_test_kaggle_digits.csv&#39;, index=False) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After submitting the csv file we get an accuracy of 0.91100 which is not that bad (unless you check the rank and realize we&amp;rsquo;re at the bottom!!).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>